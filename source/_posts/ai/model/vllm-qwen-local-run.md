---
title: å¤§æ¨¡å‹ | CPU æ¨¡å¼éƒ¨ç½²è¿è¡Œ Qwen 2.5 å¤§æ¨¡å‹
tags:
  - å¤§æ¨¡å‹
categories:
  - - äººå·¥æ™ºèƒ½
abbrlink: 51728
date: 2024-10-31 14:03:49
---

ç®€å•æ¢³ç†ä¸‹ä½¿ç”¨ vllm é€šè¿‡ CPU æ¨¡å¼ï¼Œéƒ¨ç½²è¿è¡Œ Qwen/Qwen2.5-1.5B-Instruct<!--more-->

### 1 ç¯å¢ƒå‡†å¤‡

#### 1.1 æ“ä½œç³»ç»Ÿ

ä½¿ç”¨çš„æ˜¯ `Ubuntu` ç‰ˆæœ¬å·ï¼š`24.04.1 LTS`ï¼Œ32 G å†…å­˜ï¼Œ16 æ ¸çš„ CPUï¼Œæ²¡æœ‰ GPUï¼Œè¿™é‡Œæˆ‘æ˜¯åœ¨æœ¬åœ°çš„æœåŠ¡å™¨ä¸Šå»ºç«‹äº†ä¸€å°è™šæ‹Ÿæœº

```python
# lsb_release -a

No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 24.04.1 LTS
Release:	24.04
Codename:	noble
```

#### 1.2 conda å®‰è£…

python ç¯å¢ƒï¼Œä½¿ç”¨çš„æ˜¯ `Miniconda`
å®‰è£…ç›´æ¥å‚è€ƒå®˜æ–¹æ–‡æ¡£å³å¯ï¼šhttps://docs.anaconda.com/miniconda/#quick-command-line-install

```python
mkdir -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm -rf ~/miniconda3/miniconda.sh
```

ç„¶åæ¿€æ´» miniconda

```python
conda init bash

source /root/.bashrc
```

#### 1.3 conda é•œåƒè®¾ç½®

é…ç½®é•œåƒæ˜¯ä¸ºäº†åŠ é€Ÿå®‰è£…åŒ…ï¼Œä½¿ç”¨çš„æ˜¯ `æ¸…åå¤§å­¦` é•œåƒ
æ£€æŸ¥ç”¨æˆ·ç›®å½•ä¸‹æ˜¯å¦å­˜åœ¨ `.condarc`ï¼Œå¦‚æœä¸å­˜åœ¨ï¼Œåˆ™é€šè¿‡ä»¥ä¸‹å‘½ä»¤è¿›è¡Œåˆ›å»º

```python
conda config --set show_channel_urls yes
```

ç„¶åæ‰“å¼€ .condarc æ–‡ä»¶ï¼Œè®¾ç½®æ¸…åå¤§å­¦é•œåƒ

```python
show_channel_urls: true
channels:
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
  - defaults
custom_channels:
  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
```

ä¿å­˜åï¼Œé€šè¿‡ conda info æŸ¥çœ‹é…ç½®æ˜¯å¦ç”Ÿæ•ˆ

```python
(base) root@local:~# conda info

     active environment : base
    active env location : /root/miniconda3
            shell level : 1
       user config file : /root/.condarc
 populated config files : /root/.condarc
          conda version : 24.7.1
    conda-build version : not installed
         python version : 3.12.4.final.0
                 solver : libmamba (default)
       virtual packages : __archspec=1=haswell
                          __conda=24.7.1=0
                          __glibc=2.39=0
                          __linux=6.8.0=0
                          __unix=0=0
       base environment : /root/miniconda3  (writable)
      conda av data dir : /root/miniconda3/etc/conda
  conda av metadata url : None
           channel URLs : https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/linux-64
                          https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/noarch
                          https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/linux-64
                          https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/noarch
                          https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/linux-64
                          https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/noarch
                          https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/linux-64
                          https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/noarch
                          https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/linux-64
                          https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/noarch
                          https://repo.anaconda.com/pkgs/main/linux-64
                          https://repo.anaconda.com/pkgs/main/noarch
                          https://repo.anaconda.com/pkgs/r/linux-64
                          https://repo.anaconda.com/pkgs/r/noarch
          package cache : /root/miniconda3/pkgs
                          /root/.conda/pkgs
       envs directories : /root/miniconda3/envs
                          /root/.conda/envs
               platform : linux-64
             user-agent : conda/24.7.1 requests/2.32.3 CPython/3.12.4 Linux/6.8.0-47-generic ubuntu/24.04.1 glibc/2.39 solver/libmamba conda-libmamba-solver/24.7.0 libmambapy/1.5.8 aau/0.4.4 c/. s/. e/.
                UID:GID : 0:0
             netrc file : None
           offline mode : False
```

#### 1.4 åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

åˆ›å»ºç¯å¢ƒåä¸º `ravllm` çš„è™šæ‹Ÿç¯å¢ƒï¼Œä½¿ç”¨çš„æ˜¯ `Python 3.10`ï¼Œç„¶åæ¿€æ´»

```python
conda create --name ravllM python=3.10 -y

conda activate ravllM
```

#### 1.5 å®‰è£… vllm

å› ä¸º vllm é»˜è®¤æ”¯æŒ GPUï¼Œæ‰€ä»¥éœ€è¦å®‰è£… CPU ç‰ˆæœ¬ï¼Œåˆ™éœ€è¦è‡ªè¡Œç¼–è¯‘å®‰è£…

> ğŸ”¥ å¦‚æœä½ æœ‰ GPU æ˜¾å¡ï¼Œå»ºè®®ç›´æ¥ä½¿ç”¨ GPU ç‰ˆæœ¬

```shell
pip install vllm
```

---

> ğŸ”¥ å¦‚æœä½ æ²¡æœ‰ GPU æ˜¾å¡ï¼Œå¦‚æœç”¨ CPU ç‰ˆæœ¬ï¼Œåˆ™éœ€è¦è‡ªè¡Œç¼–è¯‘å®‰è£…

```shell
# å®‰è£… gcc ç¼–è¯‘å™¨
apt update  -y
apt install -y gcc-12 g++-12 libnuma-dev
update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12
apt install -y cmake

# å…‹éš† vllm ä»“åº“
mkdir ~/codespace && cd ~/codespace
git clone https://github.com/vllm-project/vllm.git vllm

# å®‰è£… vllm ä»“åº“æ‰€éœ€çš„ä¾èµ–
cd vllm
pip install wheel packaging ninja "setuptools>=49.4.0" numpy
pip install -v -r requirements-cpu.txt --extra-index-url https://download.pytorch.org/whl/cpu

# æ‰“åŒ…å®‰è£…
VLLM_TARGET_DEVICE=cpu python setup.py install
```

### 2 å¤§æ¨¡å‹ä¸‹è½½

```shell
# å®‰è£… git ç¯å¢ƒ
apt install -y git git-lfs
git lfs install

# é€šè¿‡ git ä¸‹è½½å¤§æ¨¡å‹
mkdir -p ~/modelspace && cd ~/modelspace
git clone https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct
```

æ³¨æ„ï¼š

- å¦‚æœä» https://huggingface.co ä¸‹è½½è¾ƒæ…¢ï¼Œå¯ä»¥ä»å›½å†…çš„ https://www.modelscope.cn è¿›è¡Œä¸‹è½½
- å½“ç„¶ä¹Ÿå¯ä»¥é€šè¿‡ sdk æˆ–è€…å‘½ä»¤è¡Œçš„æ–¹å¼è¿›è¡Œä¸‹è½½ï¼Œå¯ä»¥è‡ªè¡Œå‚è€ƒå®˜æ–¹æ–‡æ¡£

clone å®Œåï¼ŒæŸ¥çœ‹ä¸‹å¤§æ¨¡å‹ç›®å½•ç»“æ„

```shell
(base) root@local:~/modelspace/Qwen2.5-1.5B-Instruct# ls -lah
total 2.9G
drwxr-xr-x 3 root root 4.0K Oct 30 06:05 .
drwxr-xr-x 5 root root 4.0K Oct 30 07:11 ..
-rw-r--r-- 1 root root  660 Oct 30 06:05 config.json
-rw-r--r-- 1 root root  242 Oct 30 06:05 generation_config.json
drwxr-xr-x 8 root root 4.0K Oct 30 06:07 .git
-rw-r--r-- 1 root root 1.5K Oct 30 06:05 .gitattributes
-rw-r--r-- 1 root root  12K Oct 30 06:05 LICENSE
-rw-r--r-- 1 root root 1.6M Oct 30 06:05 merges.txt
-rw-r--r-- 1 root root 2.9G Oct 30 06:05 model.safetensors
-rw-r--r-- 1 root root 4.9K Oct 30 06:05 README.md
-rw-r--r-- 1 root root 7.2K Oct 30 06:05 tokenizer_config.json
-rw-r--r-- 1 root root 6.8M Oct 30 06:05 tokenizer.json
-rw-r--r-- 1 root root 2.7M Oct 30 06:05 vocab.json
```

### 3 æ¨ç† - ç›´æ¥é€šè¿‡è„šæœ¬

ç›´æ¥é€šè¿‡ python è„šæœ¬æ¥è°ƒç”¨æ¨¡å‹ï¼Œè€Œä¸æ˜¯é€šè¿‡ api çš„æ–¹å¼è¿›è¡Œè°ƒç”¨

```python
import os
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['VLLM_TARGET_DEVICE'] = 'cpu'

# ä¸‹è½½çš„æ¨¡å‹æƒé‡æ–‡ä»¶ç›®å½•
model_dir = '/root/modelspace/Qwen2.5-1.5B-Instruct'

tokenizer = AutoTokenizer.from_pretrained(
    model_dir,
    local_files_only=True,
)

messages = [
    {'role': 'system', 'content': 'You are a helpful assistant.'},
    {'role': 'user', 'content': 'è·Ÿæˆ‘è®²è®²è‚¡å¸‚è¿è¡Œçš„é€»è¾‘'}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
)

llm = LLM(
    model=model_dir,
    tensor_parallel_size=1,
    device='cpu',
)

sampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=512)

outputs = llm.generate([text], sampling_params)

for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text

    print(f'æç¤ºè¯ï¼š{prompt!r}, å¤§æ¨¡å‹æ¨ç†ç»“æœè¾“å‡ºï¼š{generated_text!r}')
```

ç„¶ååœ¨æ¿€æ´»çš„ ravllm ç¯å¢ƒä¸‹è¿è¡Œæ­¤è„šæœ¬ï¼Œåœ¨ç»“æœæœªè¾“å‡ºä¹‹å‰ï¼Œçœ‹ä¸‹å†…å­˜å’Œ CPU çš„çŠ¶æ€
![cpu-status.png](cpu-status.png)

å¯ä»¥çœ‹åˆ° 16 æ ¸çš„ CPU éƒ½åœ¨åŠªåŠ›çš„å·¥ä½œã€‚

æ¨ç†ç»“æŸï¼Œæ¨ç†ç»“æœå¦‚ä¸‹

```shell
(ravllm) root@local:~/lmcode/25# python local.py
INFO 10-31 08:51:52 importing.py:13] Triton not installed; certain
GPU-related functions will not be available.
WARNING 10-31 08:52:02 config.py:421] Async output processing is
only supported for CUDA, TPU, XPU. Disabling it for other platforms.

INFO 10-31 08:52:02 llm_engine.py:240] Initializing an LLM engine
(v0.6.3.post2.dev76+g51c24c97) with config: model='/root/modelspace/
Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='/root/
modelspace/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False,
tokenizer_mode=auto, revision=None, override_neuron_config=None,
rope_scaling=None, rope_theta=None, tokenizer_revision=None,
trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768,
download_dir=None, load_format=LoadFormat.AUTO,
tensor_parallel_size=1, pipeline_parallel_size=1,
disable_custom_all_reduce=False, quantization=None,
enforce_eager=False, kv_cache_dtype=auto,
quantization_param_path=None, device_config=cpu,
decoding_config=DecodingConfig(guided_decoding_backend='outlines'),
observability_config=ObservabilityConfig(otlp_traces_endpoint=None,
collect_model_forward_time=False, collect_model_execute_time=False),
seed=0, served_model_name=/root/modelspace/Qwen2.5-1.5B-Instruct,
num_scheduler_steps=1, chunked_prefill_enabled=False
multi_step_stream_outputs=True, enable_prefix_caching=False,
use_async_output_proc=False, use_cached_outputs=False,
mm_processor_kwargs=None)

WARNING 10-31 08:52:03 cpu_executor.py:332] CUDA graph is not
supported on CPU, fallback to the eager mode.

WARNING 10-31 08:52:03 cpu_executor.py:362] Environment variable
VLLM_CPU_KVCACHE_SPACE (GB) for CPU backend is not set, using 4 by
default.
INFO 10-31 08:52:03 selector.py:193]
Cannot use _Backend.FLASH_ATTN backend on CPU.
INFO 10-31 08:52:03 selector.py:131] Using Torch SDPA backend.
INFO 10-31 08:52:03 selector.py:193]
Cannot use _Backend.FLASH_ATTN backend on CPU.
INFO 10-31 08:52:03 selector.py:131] Using Torch SDPA backend.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.43it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.43it/s]

INFO 10-31 08:52:05 cpu_executor.py:214] # CPU blocks: 9362
Processed prompts:   0%|                  | 0/1
[00:00<?, ?it/s, eProcessed prompts: 100%|â–ˆ| 1/1
[07:28<00:00 44833s Processed prompts: 100%|â–ˆ| 1/1
[07:28<00:00, 448.33s
Promptæç¤ºè¯:
'<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n
<|im_start|>user\nè·Ÿæˆ‘è®²è®²è‚¡å¸‚è¿è¡Œçš„é€»è¾‘<|im_end|>\n
<|im_start|>assistant\n',
å¤§æ¨¡å‹æ¨ç†è¾“å‡º: 'è‚¡å¸‚æ˜¯ä¸€ä¸ªå¤æ‚çš„å¸‚åœºï¼Œå…¶è¿è¡Œå—åˆ°å¤šç§
å› ç´ çš„å½±å“ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›åŸºæœ¬çš„é€»è¾‘ï¼š\n\n1. ä¾›æ±‚å…³ç³»ï¼šè‚¡ç¥¨ä»·æ ¼é€šå¸¸ç”±ä¾›ç»™å’Œéœ€æ±‚å†³å®šã€‚
å¦‚æœå¸‚åœºä¸Šæœ‰æ›´å¤šçš„äººæƒ³è¦è´­ä¹°è‚¡ç¥¨ï¼Œè€Œä¾›åº”è€…å´æœ‰é™ï¼Œé‚£ä¹ˆè‚¡ç¥¨çš„ä»·æ ¼å°±ä¼šä¸Šå‡ã€‚åä¹‹ï¼Œå¦‚
æœä¾›åº”é‡å¤§äºéœ€æ±‚é‡ï¼Œè‚¡ç¥¨çš„ä»·æ ¼å°±ä¼šä¸‹é™ã€‚\n\n2. å…¬å¸åŸºæœ¬é¢ï¼šå…¬å¸çš„è´¢åŠ¡çŠ¶å†µã€ç›ˆåˆ©èƒ½
åŠ›ã€å¢é•¿æ½œåŠ›ç­‰å› ç´ éƒ½ä¼šå½±å“è‚¡ç¥¨ä»·æ ¼ã€‚æŠ•èµ„è€…ä¼šå…³æ³¨å…¬å¸çš„ç›ˆåˆ©é¢„æµ‹ã€å¸‚ç›ˆç‡ã€è‚¡æ¯æ”¯ä»˜ç­‰
æŒ‡æ ‡ã€‚\n\n3. å¸‚åœºæƒ…ç»ªï¼šå¸‚åœºæƒ…ç»ªä¹Ÿä¼šå½±å“è‚¡ç¥¨ä»·æ ¼ã€‚ä¾‹å¦‚ï¼Œåœ¨ç»æµè¡°é€€æ—¶æœŸï¼ŒæŠ•èµ„è€…å¯èƒ½
ä¼šæ›´åŠ è°¨æ…ï¼Œä»è€Œå¯¼è‡´è‚¡ç¥¨ä»·æ ¼ä¸‹è·Œã€‚è€Œåœ¨ç»æµç¹è£æ—¶æœŸï¼ŒæŠ•èµ„è€…å¯èƒ½ä¼šæ›´åŠ ä¹è§‚ï¼Œå¯¼è‡´è‚¡ç¥¨
ä»·æ ¼ä¸Šæ¶¨ã€‚\n\n4. åˆ©ç‡æ°´å¹³ï¼šåˆ©ç‡æ°´å¹³ä¹Ÿä¼šå½±å“è‚¡ç¥¨ä»·æ ¼ã€‚å½“åˆ©ç‡ä¸Šå‡æ—¶ï¼ŒæŠ•èµ„è€…å¯èƒ½ä¼šæ›´
æ„¿æ„å°†èµ„é‡‘æŠ•èµ„äºå€ºåˆ¸ç­‰å›ºå®šæ”¶ç›Šäº§å“ï¼Œè€Œä¸æ˜¯è‚¡ç¥¨ï¼Œä»è€Œå¯¼è‡´è‚¡ç¥¨ä»·æ ¼ä¸‹è·Œã€‚ç›¸åï¼Œå½“åˆ©ç‡
ä¸‹é™æ—¶ï¼Œè‚¡ç¥¨ä»·æ ¼å¯èƒ½ä¼šä¸Šæ¶¨ã€‚\n\n5. å¤–éƒ¨äº‹ä»¶ï¼šå¤–éƒ¨äº‹ä»¶ï¼Œå¦‚è‡ªç„¶ç¾å®³ã€æ”¿ç­–å˜åŒ–ã€åœ°ç¼˜
æ”¿æ²»å†²çªç­‰ï¼Œä¹Ÿå¯èƒ½å¯¹è‚¡å¸‚äº§ç”Ÿé‡å¤§å½±å“ã€‚\n\néœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›å› ç´ å¹¶ä¸æ˜¯ç‹¬ç«‹çš„ï¼Œå®ƒä»¬
ä¹‹é—´å¯èƒ½å­˜åœ¨ç›¸äº’ä½œç”¨ã€‚æ­¤å¤–ï¼Œè‚¡å¸‚ä»·æ ¼ä¹Ÿä¼šå—åˆ°å…¶ä»–å„ç§å› ç´ çš„å½±å“ï¼Œå¦‚å…¬å¸ä¸šç»©ã€æ–°é—»æŠ¥
é“ã€å¸‚åœºé¢„æœŸç­‰ã€‚å› æ­¤ï¼ŒæŠ•èµ„è€…éœ€è¦ç»¼åˆè€ƒè™‘å„ç§å› ç´ ï¼Œå¹¶ç»“åˆè‡ªå·±çš„æŠ•èµ„ç­–ç•¥è¿›è¡Œå†³ç­–ã€‚'
```

### 4 æ¨ç† - é€šè¿‡è°ƒç”¨ api

#### 4.1 éƒ¨ç½²å¤§æ¨¡å‹ api æœåŠ¡

##### 4.1.1 éƒ¨ç½² api

ä¸ç®¡æ˜¯è„šæœ¬å®¢æˆ·ç«¯è°ƒç”¨ apiï¼Œè¿˜æ˜¯ webui çš„æ–¹å¼è¿›è¡Œè°ƒç”¨ apiï¼Œé¦–å…ˆè¦åšçš„æ˜¯éƒ¨ç½²è¿è¡Œå¤§æ¨¡å‹ api æœåŠ¡

```shell
vllm serve /root/modelspace/Qwen2.5-1.5B-Instruct
  --served-model-name Qwen/Qwen2.5-1.5B-Instruct
  --port 8000
  --host 0.0.0.0
  --device cpu
  --disable-frontend-multiprocessing
```

---

**æ³¨æ„**ï¼šä¸Šé¢çš„å‘½ä»¤ä¸­ï¼Œé‡ç‚¹å…³æ³¨ä¸‹ `--disable-frontend-multiprocessing` è¿™ä¸ªå‚æ•°é€‰é¡¹ï¼Œå¦‚æœä¸æŒ‡å®šï¼Œå®¢æˆ·ç«¯è°ƒç”¨ api çš„æ—¶å€™ä¼šæŠ¥é”™ã€‚å¯èƒ½è·Ÿæˆ‘ä½¿ç”¨çš„æ¨¡å‹çš„å‚æ•°å¤§å°æœ‰å…³ç³»ï¼Œå¦‚æœæ˜¯ 0.5Bï¼Œå°±ä¸ä¼šæŠ¥é”™

---

```python
INFO 11-01 01:16:06 engine.py:297] Added request
chat-027710c78bb6449cb547ea684c82e5c0.

ERROR 11-01 01:16:33 client.py:262] RuntimeError('Engine loop has died')
ERROR 11-01 01:16:33 client.py:262] Traceback (most recent call last):

ERROR 11-01 01:16:33 client.py:262]   File "/root/miniconda3/envs/
ravllm/lib/python3.10/site-packages/vllm-0.6.3.post2.dev76+g51c24c97.
cpu-py3.10-linux-x86_64.egg/vllm/engine/multiprocessing/client.py",
line 150, in run_heartbeat_loop

ERROR 11-01 01:16:33 client.py:262]     await self._check_success(
ERROR 11-01 01:16:33 client.py:262]   File "/root/miniconda3/envs/
ravllm/lib/python3.10/site-packages/vllm-0.6.3.post2.dev76+g51c24c97.
cpu-py3.10-linux-x86_64.egg/vllm/engine/multiprocessing/client.py",
line 326, in _check_success

ERROR 11-01 01:16:33 client.py:262]     raise response
ERROR 11-01 01:16:33 client.py:262] RuntimeError: Engine loop has died
CRITICAL 11-01 01:16:43 launcher.py:99] MQLLMEngine is already dead,
terminating server process

INFO:     127.0.0.1:42718 - "POST /v1/chat/completions HTTP/1.1"
                            500 Internal Server Error
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [124376]

(ravllm) root@local:~# /root/miniconda3/envs/ravllm/lib/python3.10/
multiprocessing/resource_tracker.py:224: UserWarning:
resource_tracker: There appear to be 1 leaked semaphore objects to
clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
```

ğŸ†˜ ğŸ†˜ ğŸ†˜
**æŸ¥é˜…äº†èµ„æ–™ï¼Œç›®å‰è¿˜æ²¡æœ‰æ‰¾å‡ºå…·ä½“åŸå› ï¼ˆç½‘ä¸Šçš„èµ„æ–™è¯´ä»€ä¹ˆçš„éƒ½æœ‰ï¼Œéƒ½ä¸èƒ½è§£å†³é—®é¢˜ï¼‰**

ğŸš€ æ­£å¸¸å¯åŠ¨æœåŠ¡åï¼Œè¿è¡Œå¦‚ä¸‹ï¼š

```python
(ravllm) root@local:~# vllm serve /root/modelspace/Qwen2.5-1.5B-Instruct --served-model-name Qwen/Qwen2.5-1.5B-Instruct --port 8000 --host 0.0.0.0 --device cpu --disable-frontend-multiprocessing
INFO 11-01 01:33:42 importing.py:13] Triton not installed; certain GPU-related functions will not be available.
INFO 11-01 01:33:45 api_server.py:528] vLLM API server version 0.6.3.post2.dev76+g51c24c97
INFO 11-01 01:33:45 api_server.py:529] args: Namespace(subparser='serve', model_tag='/root/modelspace/Qwen2.5-1.5B-Instruct', config='', host='0.0.0.0', port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=True, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/root/modelspace/Qwen2.5-1.5B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='cpu', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['Qwen/Qwen2.5-1.5B-Instruct'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7602e53ec700>)
WARNING 11-01 01:33:52 arg_utils.py:1038] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
WARNING 11-01 01:33:52 config.py:421] Async output processing is only supported for CUDA, TPU, XPU. Disabling it for other platforms.
INFO 11-01 01:33:52 llm_engine.py:240] Initializing an LLM engine (v0.6.3.post2.dev76+g51c24c97) with config: model='/root/modelspace/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='/root/modelspace/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cpu, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 11-01 01:33:53 cpu_executor.py:332] CUDA graph is not supported on CPU, fallback to the eager mode.
WARNING 11-01 01:33:53 cpu_executor.py:362] Environment variable VLLM_CPU_KVCACHE_SPACE (GB) for CPU backend is not set, using 4 by default.
INFO 11-01 01:33:56 importing.py:13] Triton not installed; certain GPU-related functions will not be available.
(VllmWorkerProcess pid=124634) INFO 11-01 01:33:59 selector.py:193] Cannot use _Backend.FLASH_ATTN backend on CPU.
(VllmWorkerProcess pid=124634) INFO 11-01 01:33:59 selector.py:131] Using Torch SDPA backend.
(VllmWorkerProcess pid=124634) INFO 11-01 01:33:59 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
(VllmWorkerProcess pid=124634) INFO 11-01 01:33:59 selector.py:193] Cannot use _Backend.FLASH_ATTN backend on CPU.
(VllmWorkerProcess pid=124634) INFO 11-01 01:33:59 selector.py:131] Using Torch SDPA backend.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.43it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.43it/s]
(VllmWorkerProcess pid=124634)
INFO 11-01 01:34:00 cpu_executor.py:214] # CPU blocks: 9362
WARNING 11-01 01:34:01 serving_embedding.py:200] embedding_mode is False. Embedding API will not work.
INFO 11-01 01:34:01 launcher.py:19] Available routes are:
INFO 11-01 01:34:01 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET
INFO 11-01 01:34:01 launcher.py:27] Route: /docs, Methods: HEAD, GET
INFO 11-01 01:34:01 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 11-01 01:34:01 launcher.py:27] Route: /redoc, Methods: HEAD, GET
INFO 11-01 01:34:01 launcher.py:27] Route: /health, Methods: GET
INFO 11-01 01:34:01 launcher.py:27] Route: /tokenize, Methods: POST
INFO 11-01 01:34:01 launcher.py:27] Route: /detokenize, Methods: POST
INFO 11-01 01:34:01 launcher.py:27] Route: /v1/models, Methods: GET
INFO 11-01 01:34:01 launcher.py:27] Route: /version, Methods: GET
INFO 11-01 01:34:01 launcher.py:27] Route: /v1/chat/completions, Methods: POST
INFO 11-01 01:34:01 launcher.py:27] Route: /v1/completions, Methods: POST
INFO 11-01 01:34:01 launcher.py:27] Route: /v1/embeddings, Methods: POST
INFO:     Started server process [124595]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on socket ('0.0.0.0', 8000) (Press CTRL+C to quit)
INFO 11-01 01:34:11 metrics.py:363] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 11-01 01:34:21 metrics.py:363] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
```

##### 4.1.2 æŸ¥çœ‹æ¥å£è·¯ç”±

å¯åŠ¨åï¼Œå¯ä»¥çœ‹åˆ°æš´éœ²å‡ºçš„æ¥å£è·¯ç”±

```python
INFO 11-01 01:34:01 launcher.py:19] Available routes are:
INFO 11-01 01:34:01 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET
INFO 11-01 01:34:01 launcher.py:27] Route: /docs, Methods: HEAD, GET
INFO 11-01 01:34:01 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 11-01 01:34:01 launcher.py:27] Route: /redoc, Methods: HEAD, GET
INFO 11-01 01:34:01 launcher.py:27] Route: /health, Methods: GET
INFO 11-01 01:34:01 launcher.py:27] Route: /tokenize, Methods: POST
INFO 11-01 01:34:01 launcher.py:27] Route: /detokenize, Methods: POST
INFO 11-01 01:34:01 launcher.py:27] Route: /v1/models, Methods: GET
INFO 11-01 01:34:01 launcher.py:27] Route: /version, Methods: GET
INFO 11-01 01:34:01 launcher.py:27] Route: /v1/chat/completions, Methods: POST
INFO 11-01 01:34:01 launcher.py:27] Route: /v1/completions, Methods: POST
INFO 11-01 01:34:01 launcher.py:27] Route: /v1/embeddings, Methods: POST
```

åœ¨æµè§ˆå™¨ä¸­éªŒè¯ä¸‹ä¸€ä¸ªæ¥å£ï¼Œè¿™é‡Œé€‰æ‹© `/v1/models`ï¼Œåœ¨æµè§ˆå™¨ä¸­è¾“å…¥å®Œæ•´çš„åœ°å€ï¼š`http://192.168.1.14:8000/v1/models`ï¼Œè¾“å‡ºç»“æœå¦‚ä¸‹

```json
{
  "object": "list",
  "data": [
    {
      "id": "Qwen/Qwen2.5-1.5B-Instruct",
      "object": "model",
      "created": 1730425425,
      "owned_by": "vllm",
      "root": "/root/modelspace/Qwen2.5-1.5B-Instruct",
      "parent": null,
      "max_model_len": 32768,
      "permission": [
        {
          "id": "modelperm-c05e0ff5d1494dbb9668f2b0bbed42c4",
          "object": "model_permission",
          "created": 1730425425,
          "allow_create_engine": false,
          "allow_sampling": true,
          "allow_logprobs": true,
          "allow_search_indices": false,
          "allow_view": true,
          "allow_fine_tuning": false,
          "organization": "*",
          "group": null,
          "is_blocking": false
        }
      ]
    }
  ]
}
```

#### 4.2 è„šæœ¬å®¢æˆ·ç«¯è°ƒç”¨ api

python è„šæœ¬å¦‚ä¸‹ï¼š

```python
from openai import OpenAI

openai_api_key = "EMPTY"
openai_api_base = "http://localhost:8000/v1"

client = OpenAI(
    api_key=openai_api_key,
    base_url=openai_api_base,
)

chat_response = client.chat.completions.create(
    model='Qwen/Qwen2.5-1.5B-Instruct',
    messages=[
        {
          "role": "system",
          "content": "You are Qwen. You are a helpful assistant.",
        },
        {
          "role": "user",
          "content": "è¯¦ç»†è®²è§£ä¸‹è‚¡å¸‚è¿è¡Œçš„åŸç†",
        },
    ],
    temperature=0.7,
    top_p=0.8,
    max_tokens=512,
    extra_body={
        "repetition_penalty": 1.05,
    },
    timeout=1800,
)

print("Chat response:", chat_response)
```

è¿è¡Œè„šæœ¬åï¼ŒæŸ¥çœ‹ä¸‹ api æœåŠ¡ç«¯çš„æ—¥å¿—

```python
INFO 11-01 01:50:17 logger.py:37] Received request
chat-2eea5af7a0f9416abdd7b625863de374: prompt: '<|im_start|
>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful
assistant.<|im_end|>\n<|im_start|>user\nè¯¦ç»†è®²è§£ä¸‹è‚¡å¸‚è¿è¡Œçš„åŸç†<|im_end|
>\n<|im_start|>assistant\n', params: SamplingParams(n=1,
presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.05,
temperature=0.7, top_p=0.8, top_k=-1, min_p=0.0, seed=None, stop=[],
stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False,
max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None,
skip_special_tokens=True, spaces_between_special_tokens=True,
truncate_prompt_tokens=None), guided_decoding=None, prompt_token_ids:
[151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364,
14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872,
198, 100700, 105250, 16872, 104908, 104001, 9370, 105318, 151645, 198,
151644, 77091, 198], lora_request: None, prompt_adapter_request: None.

INFO 11-01 01:50:17 async_llm_engine.py:207] Added request chat-2eea5af7a0f9416abdd7b625863de374.
INFO 11-01 01:50:21 metrics.py:363] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 11-01 01:50:31 metrics.py:363] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 11-01 01:50:41 metrics.py:363] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 11-01 01:50:51 metrics.py:363] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 11-01 01:51:01 metrics.py:363] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 11-01 01:51:11 metrics.py:363] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 11-01 01:51:19 metrics.py:363] Avg prompt throughput: 4.6 tokens/s, Avg generation throughput: 0.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 11-01 01:51:24 metrics.py:363] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 11-01 01:51:29 metrics.py:363] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 11-01 01:51:35 metrics.py:363] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 11-01 01:51:40 metrics.py:363] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 11-01 01:51:46 metrics.py:363] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
```

å¯ä»¥çœ‹åˆ°æœåŠ¡ç«¯å·²ç»æ¥æ”¶åˆ°è¯·æ±‚ï¼Œå¹¶å¼€å§‹é€æ­¥çš„è¿›è¡Œå¤„ç†ï¼Œå¤„ç†ç»“æŸåï¼Œå¯ä»¥çœ‹åˆ°æ¥å£è¿”å›ç»™å®¢æˆ·ç«¯çš„ç»“æœï¼Œè¿™é‡Œåœ¨å®¢æˆ·ç«¯è„šæœ¬ä¸­è¿›è¡Œæ‰“å°ï¼ˆè¿™é‡Œç­‰å¾…çš„æ—¶é—´æœ‰ç‚¹ä¹…ï¼Œå¦‚æœæ˜¯ 0.5B å°±å¾ˆå¿«ï¼‰ï¼Œè¿”å›ç»“æœå¦‚ä¸‹ï¼š

```python
(ravllm) root@local:~/lmcode/25# python requ.py
Chat response: ChatCompletion
(id='chat-2eea5af7a0f9416abdd7b625863de374', choices=[Choice
(finish_reason='stop', index=0, logprobs=None,
message=ChatCompletionMessage(content='è‚¡å¸‚è¿è¡Œçš„åŸç†ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š
\n\n1. ä¾›éœ€å…³ç³»ï¼šè‚¡å¸‚æ˜¯å•†å“å’ŒæœåŠ¡çš„ä»·æ ¼å†³å®šæœºåˆ¶ï¼Œè‚¡ç¥¨ä»·æ ¼ç”±å¸‚åœºä¸Šçš„ä¾›æ±‚å…³ç³»å†³å®šã€‚å½“
å¸‚åœºä¸Šæœ‰æ›´å¤šçš„äººæƒ³è´­ä¹°è‚¡ç¥¨ï¼Œè€Œä¾›ä¸åº”æ±‚æ—¶ï¼Œè‚¡ç¥¨ä»·æ ¼ä¼šä¸Šæ¶¨ï¼›åä¹‹ï¼Œå½“å¸‚åœºä¸Šæœ‰æ›´å¤šçš„è‚¡ç¥¨
ä¾›åº”ï¼Œè€Œéœ€æ±‚å°äºä¾›ç»™æ—¶ï¼Œè‚¡ç¥¨ä»·æ ¼ä¼šä¸‹è·Œã€‚\n\n2. åˆ©ç‡å˜åŠ¨ï¼šåˆ©ç‡çš„ä¸Šå‡é€šå¸¸ä¼šå¯¼è‡´è‚¡ç¥¨ä»·
æ ¼ä¸‹é™ï¼Œå› ä¸ºé«˜åˆ©ç‡å¯èƒ½ä¼šé™ä½æŠ•èµ„è€…çš„é£é™©åå¥½ï¼Œå¯¼è‡´æŠ•èµ„è€…æ›´æ„¿æ„å°†èµ„é‡‘æŠ•èµ„äºå€ºåˆ¸ç­‰ä½é£
é™©èµ„äº§ï¼Œè€Œä¸æ˜¯è‚¡ç¥¨ã€‚ç›¸åï¼Œå¦‚æœåˆ©ç‡ä¸‹é™ï¼Œè‚¡ç¥¨ä»·æ ¼é€šå¸¸ä¼šä¸Šæ¶¨ï¼Œå› ä¸ºæŠ•èµ„è€…å¯èƒ½æ›´æ„¿æ„å°†èµ„
é‡‘æŠ•èµ„äºè‚¡ç¥¨ç­‰é«˜é£é™©èµ„äº§ã€‚\n\n3. å…¬å¸ä¸šç»©ï¼šå…¬å¸çš„ç›ˆåˆ©å’Œä¸šç»©æ˜¯å½±å“è‚¡ç¥¨ä»·æ ¼çš„é‡è¦å› 
ç´ ã€‚å½“ä¸€å®¶å…¬å¸ç›ˆåˆ©å¢é•¿æ—¶ï¼Œå…¶è‚¡ç¥¨ä»·æ ¼é€šå¸¸ä¼šä¸Šæ¶¨ï¼Œå› ä¸ºæŠ•èµ„è€…è®¤ä¸ºè¯¥å…¬å¸æœ‰æ›´é«˜çš„æœªæ¥æ”¶ç›Š
æ½œåŠ›ã€‚ç›¸åï¼Œå¦‚æœå…¬å¸ä¸šç»©ä¸‹é™ï¼Œè‚¡ç¥¨ä»·æ ¼é€šå¸¸ä¼šä¸‹è·Œã€‚\n\n4. ç«äº‰ä¸å¹¶è´­ï¼šåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œ
ç«äº‰å’Œå¹¶è´­ä¹Ÿä¼šå½±å“è‚¡å¸‚çš„è¿è¡Œã€‚ä¾‹å¦‚ï¼Œå¦‚æœä¸¤å®¶æˆ–å¤šå®¶å…¬å¸åˆå¹¶ï¼Œå¯èƒ½ä¼šå¯¼è‡´å¸‚åœºä¸Šå¯ä¾›äº¤æ˜“
çš„è‚¡ç¥¨æ•°é‡å‡å°‘ï¼Œä»è€Œæ¨é«˜è‚¡ç¥¨ä»·æ ¼ã€‚ç›¸åï¼Œå¦‚æœä¸€å®¶å…¬å¸è¢«å¦ä¸€å®¶æ”¶è´­ï¼Œå¯èƒ½ä¼šå¯¼è‡´è¯¥å…¬å¸è‚¡
ç¥¨ä»·æ ¼ä¸‹è·Œï¼Œå› ä¸ºæŠ•èµ„è€…è®¤ä¸ºè¯¥å…¬å¸å¤±å»äº†éƒ¨åˆ†ä»·å€¼ã€‚\n\n5. æ”¿ç­–ä¸æ³•è§„ï¼šæ”¿åºœçš„æ”¿ç­–å’Œæ³•è§„
ä¹Ÿå¯èƒ½å½±å“è‚¡å¸‚çš„è¿è¡Œã€‚ä¾‹å¦‚ï¼Œæ”¿åºœå¯èƒ½ä¼šå‡ºå°æ”¿ç­–æ¥åˆºæ¿€ç»æµå¢é•¿ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´è‚¡å¸‚ä¸Šæ¶¨ã€‚
ç›¸åï¼Œå¦‚æœæ”¿åºœå‡ºå°äº†ç´§ç¼©æ€§çš„æ”¿ç­–ï¼Œå¯èƒ½ä¼šå¯¼è‡´è‚¡å¸‚ä¸‹è·Œã€‚\n\nä»¥ä¸Šå°±æ˜¯è‚¡å¸‚è¿è¡Œçš„åŸºæœ¬åŸ
ç†ï¼Œå½“ç„¶ï¼Œè‚¡å¸‚è¿è¡Œå—åˆ°è®¸å¤šå…¶ä»–å› ç´ çš„å½±å“ï¼Œä¾‹å¦‚ç»æµçŠ¶å†µã€å›½é™…å½¢åŠ¿ã€æ”¿æ²»å±€åŠ¿ç­‰ç­‰ã€‚',
refusal=None, role='assistant', audio=None, function_call=None,
tool_calls=[]), stop_reason=None)], created=1730425817, model='Qwen/
Qwen2.5-1.5B-Instruct', object='chat.completion', service_tier=None,
system_fingerprint=None, usage=CompletionUsage(completion_tokens=345,
prompt_tokens=36, total_tokens=381, completion_tokens_details=None,
prompt_tokens_details=None), prompt_logprobs=None)
```

åˆ°è¿™é‡Œå¯ä»¥ç®€å•çš„åº†ç¥ä¸‹äº†ï¼Œæ¯•ç«Ÿå·²ç»å¯ä»¥ä½¿ç”¨æœ¬åœ°éƒ¨ç½²è¿è¡Œçš„å¤§æ¨¡å‹äº†ã€‚

#### 4.3 é€šè¿‡ WebUI è°ƒç”¨ api

è¿™é‡Œä½¿ç”¨äº† gradio

```shell
pip install gradio
```

```python
import argparse
import gradio as gr
import requests
import json

def query_model(prompt):
    api_url = "http://127.0.0.1:8000/v1/chat/completions"
    headers = {"User-Agent": "vLLM Client"}
    pload = {
        "model": "Qwen/Qwen2.5-1.5B-Instruct",
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 128,
        "stream": True
    }

    try:
        # å‘é€è¯·æ±‚ï¼Œè®¾ç½®è¶…æ—¶æ—¶é—´ä¸º 1800 ç§’
        response = requests.post(api_url, headers=headers, json=pload, stream=True, timeout=1800)

        result = ""
        # å¤„ç†æµå¼è¿”å›çš„å“åº”
        for chunk in response.iter_lines(chunk_size=8192, decode_unicode=False):
            if chunk:
                chunk_str = chunk.decode("utf-8")
                print(chunk_str)

                # å»é™¤å¼€å¤´çš„ "data: " éƒ¨åˆ†ï¼Œç¡®ä¿æˆ‘ä»¬åªå¤„ç† JSON éƒ¨åˆ†
                if chunk_str.startswith("data: "):
                    chunk_str = chunk_str[6:].strip()

                # ç¡®ä¿ä¸å¤„ç† keep-alive çš„å¿ƒè·³æ¶ˆæ¯
                if chunk_str == "[DONE]":
                    break

                # è§£æ JSON æ•°æ®
                try:
                    data = json.loads(chunk_str)
                    # ä»å“åº”ä¸­æå–å†…å®¹
                    if "choices" in data:
                        delta = data["choices"][0]["delta"]
                        if "content" in delta:
                            result += delta["content"]
                except json.JSONDecodeError:
                    print(f"æ— æ³•è§£æçš„ JSON æ•°æ®ï¼š{chunk_str}")

        return result
    except requests.exceptions.Timeout:
        return "è¯·æ±‚è¶…æ—¶ï¼Œè¯·é‡è¯•ã€‚"
    except Exception as e:
        return f"è¯·æ±‚å¤±è´¥ï¼š{e}"

# æ„å»º Gradio UI
def build_demo():
    with gr.Blocks() as demo:
        gr.Markdown("# Qwen/Qwen2.5-1.5B-Instruct æ¨¡å‹äº¤äº’ç•Œé¢")
        prompt = gr.Textbox(label="è¾“å…¥æç¤º", placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜æˆ–æç¤º...")
        result = gr.Textbox(label="æ¨¡å‹å“åº”")

        # ä¿®æ­£ prompt.submit() è°ƒç”¨ï¼Œåªä¼ é€’ Gradio ç»„ä»¶
        prompt.submit(query_model, inputs=prompt, outputs=result)
    return demo

# ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--host", type=str, default="0.0.0.0")
    parser.add_argument("--port", type=int, default=8001)
    args = parser.parse_args()

    demo = build_demo()
    demo.queue().launch(server_name=args.host, server_port=args.port, share=True)
```

ç„¶åè¿è¡ŒæœåŠ¡ï¼Œè¿™é‡Œæé†’ï¼Œä¸èƒ½åˆ›å»ºåˆ†äº«é“¾æ¥ï¼Œå¦‚æœæƒ³è¦åˆ›å»ºåˆ†äº«é“¾æ¥ï¼Œå°±æŒ‰ç…§ä¸‹é¢çš„æ­¥éª¤è¿›è¡Œæ“ä½œå³å¯ã€‚

```shell
(ravllm) root@local:~/lmcode/25# python ui.py
* Running on local URL:  http://0.0.0.0:8001

Could not create share link. Missing file: /root/miniconda3/envs/ravllm/lib/python3.10/site-packages/gradio/frpc_linux_amd64_v0.3.

Please check your internet connection. This can happen if your antivirus software blocks the download of this file. You can install manually by following these steps:

1. Download this file: https://cdn-media.huggingface.co/frpc-gradio-0.3/frpc_linux_amd64
2. Rename the downloaded file to: frpc_linux_amd64_v0.3
3. Move the file to this location: /root/miniconda3/envs/ravllm/lib/python3.10/site-packages/gradio
```

æŒ‰ç…§ä¸Šé¢çš„æ­¥éª¤ï¼Œä¸‹è½½ç§»åŠ¨ `frpc_linux_amd64`

```shell
cd ~
wget https://cdn-media.huggingface.co/frpc-gradio-0.3/frpc_linux_amd64
mv frpc_linux_amd64 frpc_linux_amd64_v0.3
mv frpc_linux_amd64_v0.3 /root/miniconda3/envs/ravllm/lib/python3.10/site-packages/gradio
```

ç„¶åé‡æ–°å¯åŠ¨æœåŠ¡ï¼Œåœ¨æµè§ˆå™¨ä¸­è¿›è¡Œè®¿é—®ï¼Œå¯ä»¥çœ‹åˆ°ä»¥ä¸‹ç•Œé¢ï¼Œç„¶åå°±å¯ä»¥è¿›è¡Œäº¤äº’äº†ã€‚

![webui.png](webui.png)

è¿™é‡Œæˆªå–äº†å¤„ç†è¿‡ç¨‹ä¸­çš„ä¸€æ®µæ—¥å¿—ï¼Œå¯ä»¥çœ‹åˆ°è¯·æ±‚é€šè¿‡ `stream` çš„æ–¹å¼ï¼Œåƒç®¡é“ä¸­çš„æµæ°´ä¸€æ ·

```python
data: {"id":"chat-f434de356a7d4e42b5ac89ea4d8d9b91","object":"chat.completion.chunk","created":1730427228,"model":"Qwen/Qwen2.5-1.5B-Instruct","choices":[{"index":0,"delta":{"role":"assistant","content":""},"logprobs":null,"finish_reason":null}]}
data: {"id":"chat-f434de356a7d4e42b5ac89ea4d8d9b91","object":"chat.completion.chunk","created":1730427228,"model":"Qwen/Qwen2.5-1.5B-Instruct","choices":[{"index":0,"delta":{"content":"èŠ¯ç‰‡"},"logprobs":null,"finish_reason":null}]}
data: {"id":"chat-f434de356a7d4e42b5ac89ea4d8d9b91","object":"chat.completion.chunk","created":1730427228,"model":"Qwen/Qwen2.5-1.5B-Instruct","choices":[{"index":0,"delta":{"content":"çš„"},"logprobs":null,"finish_reason":null}]}
data: {"id":"chat-f434de356a7d4e42b5ac89ea4d8d9b91","object":"chat.completion.chunk","created":1730427228,"model":"Qwen/Qwen2.5-1.5B-Instruct","choices":[{"index":0,"delta":{"content":"åˆ¶ä½œ"},"logprobs":null,"finish_reason":null}]}
data: {"id":"chat-f434de356a7d4e42b5ac89ea4d8d9b91","object":"chat.completion.chunk","created":1730427228,"model":"Qwen/Qwen2.5-1.5B-Instruct","choices":[{"index":0,"delta":{"content":"åŸç†"},"logprobs":null,"finish_reason":null}]}
data: {"id":"chat-f434de356a7d4e42b5ac89ea4d8d9b91","object":"chat.completion.chunk","created":1730427228,"model":"Qwen/Qwen2.5-1.5B-Instruct","choices":[{"index":0,"delta":{"content":"æ˜¯ä¸€é¡¹"},"logprobs":null,"finish_reason":null}]}
data: {"id":"chat-f434de356a7d4e42b5ac89ea4d8d9b91","object":"chat.completion.chunk","created":1730427228,"model":"Qwen/Qwen2.5-1.5B-Instruct","choices":[{"index":0,"delta":{"content":"éå¸¸"},"logprobs":null,"finish_reason":null}]}
data: {"id":"chat-f434de356a7d4e42b5ac89ea4d8d9b91","object":"chat.completion.chunk","created":1730427228,"model":"Qwen/Qwen2.5-1.5B-Instruct","choices":[{"index":0,"delta":{"content":"å¤æ‚"},"logprobs":null,"finish_reason":null}]}
```

æœ¬åœ°éƒ¨ç½²çš„è¿™ä¸ªå¤§æ¨¡å‹ï¼Œè¿˜æœ‰äº›ç¼ºç‚¹ï¼Œä¾‹å¦‚ä¸èƒ½ç†è§£ä¸Šä¸‹æ–‡ã€‚
å¦‚æœéƒ¨ç½²æ˜¯åœ¨æœ‰ GPU æ˜¾å¡çš„æœåŠ¡å™¨ä¸Šï¼Œéƒ¨ç½²æ›´å¿«æ·ï¼Œæ¨¡å‹æ¨ç†æ›´å¿«ã€‚

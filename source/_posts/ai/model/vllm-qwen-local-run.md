---
title: å¤§æ¨¡å‹ | CPU æ¨¡å¼éƒ¨ç½²è¿è¡Œ Qwen 2.5 å¤§æ¨¡å‹
tags:
  - å¤§æ¨¡å‹
categories:
  - - äººå·¥æ™ºèƒ½
abbrlink: 51728
date: 2024-10-31 14:03:49
---

ç®€å•æ¢³ç†ä¸‹ä½¿ç”¨ vllm é€šè¿‡ CPU æ¨¡å¼ï¼Œéƒ¨ç½²è¿è¡Œ Qwen/Qwen2.5-1.5B-Instruct<!--more-->

### 1 ç¯å¢ƒå‡†å¤‡

#### 1.1 æ“ä½œç³»ç»Ÿ

ä½¿ç”¨çš„æ˜¯ `Ubuntu` ç‰ˆæœ¬å·ï¼š`24.04.1 LTS`

```python
# lsb_release -a

No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 24.04.1 LTS
Release:	24.04
Codename:	noble
```

#### 1.2 conda å®‰è£…

python ç¯å¢ƒï¼Œä½¿ç”¨çš„æ˜¯ `Miniconda`
å®‰è£…ç›´æ¥å‚è€ƒå®˜æ–¹æ–‡æ¡£å³å¯ï¼šhttps://docs.anaconda.com/miniconda/#quick-command-line-install

```python
mkdir -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm -rf ~/miniconda3/miniconda.sh
```

ç„¶åæ¿€æ´» miniconda

```python
conda init bash

source /root/.bashrc
```

#### 1.3 conda é•œåƒè®¾ç½®

é…ç½®é•œåƒæ˜¯ä¸ºäº†åŠ é€Ÿå®‰è£…åŒ…ï¼Œä½¿ç”¨çš„æ˜¯ `æ¸…åå¤§å­¦` é•œåƒ
æ£€æŸ¥ç”¨æˆ·ç›®å½•ä¸‹æ˜¯å¦å­˜åœ¨ `.condarc`ï¼Œå¦‚æœä¸å­˜åœ¨ï¼Œåˆ™é€šè¿‡ä»¥ä¸‹å‘½ä»¤è¿›è¡Œåˆ›å»º

```python
conda config --set show_channel_urls yes
```

ç„¶åæ‰“å¼€ .condarc æ–‡ä»¶ï¼Œè®¾ç½®æ¸…åå¤§å­¦é•œåƒ

```python
show_channel_urls: true
channels:
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
  - defaults
custom_channels:
  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
```

ä¿å­˜åï¼Œé€šè¿‡ conda info æŸ¥çœ‹é…ç½®æ˜¯å¦ç”Ÿæ•ˆ

```python
(base) root@local:~# conda info

     active environment : base
    active env location : /root/miniconda3
            shell level : 1
       user config file : /root/.condarc
 populated config files : /root/.condarc
          conda version : 24.7.1
    conda-build version : not installed
         python version : 3.12.4.final.0
                 solver : libmamba (default)
       virtual packages : __archspec=1=haswell
                          __conda=24.7.1=0
                          __glibc=2.39=0
                          __linux=6.8.0=0
                          __unix=0=0
       base environment : /root/miniconda3  (writable)
      conda av data dir : /root/miniconda3/etc/conda
  conda av metadata url : None
           channel URLs : https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/linux-64
                          https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/noarch
                          https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/linux-64
                          https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/noarch
                          https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/linux-64
                          https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/noarch
                          https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/linux-64
                          https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/noarch
                          https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/linux-64
                          https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/noarch
                          https://repo.anaconda.com/pkgs/main/linux-64
                          https://repo.anaconda.com/pkgs/main/noarch
                          https://repo.anaconda.com/pkgs/r/linux-64
                          https://repo.anaconda.com/pkgs/r/noarch
          package cache : /root/miniconda3/pkgs
                          /root/.conda/pkgs
       envs directories : /root/miniconda3/envs
                          /root/.conda/envs
               platform : linux-64
             user-agent : conda/24.7.1 requests/2.32.3 CPython/3.12.4 Linux/6.8.0-47-generic ubuntu/24.04.1 glibc/2.39 solver/libmamba conda-libmamba-solver/24.7.0 libmambapy/1.5.8 aau/0.4.4 c/. s/. e/.
                UID:GID : 0:0
             netrc file : None
           offline mode : False
```

#### 1.4 åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

åˆ›å»ºç¯å¢ƒåä¸º `ravllm` çš„è™šæ‹Ÿç¯å¢ƒï¼Œä½¿ç”¨çš„æ˜¯ `Python 3.10`ï¼Œç„¶åæ¿€æ´»

```python
conda create --name ravllM python=3.10 -y

conda activate ravllM
```

#### 1.5 å®‰è£… vllm

å› ä¸º vllm é»˜è®¤æ”¯æŒ GPUï¼Œæ‰€ä»¥éœ€è¦å®‰è£… CPU ç‰ˆæœ¬ï¼Œåˆ™éœ€è¦è‡ªè¡Œç¼–è¯‘å®‰è£…

> ğŸ”¥ å¦‚æœä½ æœ‰ GPU æ˜¾å¡ï¼Œå»ºè®®ç›´æ¥ä½¿ç”¨ GPU ç‰ˆæœ¬

```shell
pip install vllm
```

---

> ğŸ”¥ å¦‚æœä½ æ²¡æœ‰ GPU æ˜¾å¡ï¼Œå¦‚æœç”¨ CPU ç‰ˆæœ¬ï¼Œåˆ™éœ€è¦è‡ªè¡Œç¼–è¯‘å®‰è£…

```shell
# å®‰è£… gcc ç¼–è¯‘å™¨
apt update  -y
apt install -y gcc-12 g++-12 libnuma-dev
update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12
apt install -y cmake

# å…‹éš† vllm ä»“åº“
mkdir ~/codespace && cd ~/codespace
git clone https://github.com/vllm-project/vllm.git vllm

# å®‰è£… vllm ä»“åº“æ‰€éœ€çš„ä¾èµ–
cd vllm
pip install wheel packaging ninja "setuptools>=49.4.0" numpy
pip install -v -r requirements-cpu.txt --extra-index-url https://download.pytorch.org/whl/cpu

# æ‰“åŒ…å®‰è£…
VLLM_TARGET_DEVICE=cpu python setup.py install
```

### 2 å¤§æ¨¡å‹ä¸‹è½½

```shell
# å®‰è£… git ç¯å¢ƒ
apt install -y git git-lfs
git lfs install

# é€šè¿‡ git ä¸‹è½½å¤§æ¨¡å‹
mkdir -p ~/modelspace && cd ~/modelspace
git clone https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct Qwen2.5-1.5B-Instruct
```

æ³¨æ„ï¼š

- å¦‚æœä» https://huggingface.co ä¸‹è½½è¾ƒæ…¢ï¼Œå¯ä»¥ä»å›½å†…çš„ https://www.modelscope.cn è¿›è¡Œä¸‹è½½
- å½“ç„¶ä¹Ÿå¯ä»¥é€šè¿‡ sdk æˆ–è€…å‘½ä»¤è¡Œçš„æ–¹å¼è¿›è¡Œä¸‹è½½ï¼Œå¯ä»¥è‡ªè¡Œå‚è€ƒå®˜æ–¹æ–‡æ¡£

clone å®Œåï¼ŒæŸ¥çœ‹ä¸‹å¤§æ¨¡å‹ç›®å½•ç»“æ„

```shell
(base) root@local:~/modelspace/Qwen2.5-1.5B-Instruct# ls -lah
total 2.9G
drwxr-xr-x 3 root root 4.0K Oct 30 06:05 .
drwxr-xr-x 5 root root 4.0K Oct 30 07:11 ..
-rw-r--r-- 1 root root  660 Oct 30 06:05 config.json
-rw-r--r-- 1 root root  242 Oct 30 06:05 generation_config.json
drwxr-xr-x 8 root root 4.0K Oct 30 06:07 .git
-rw-r--r-- 1 root root 1.5K Oct 30 06:05 .gitattributes
-rw-r--r-- 1 root root  12K Oct 30 06:05 LICENSE
-rw-r--r-- 1 root root 1.6M Oct 30 06:05 merges.txt
-rw-r--r-- 1 root root 2.9G Oct 30 06:05 model.safetensors
-rw-r--r-- 1 root root 4.9K Oct 30 06:05 README.md
-rw-r--r-- 1 root root 7.2K Oct 30 06:05 tokenizer_config.json
-rw-r--r-- 1 root root 6.8M Oct 30 06:05 tokenizer.json
-rw-r--r-- 1 root root 2.7M Oct 30 06:05 vocab.json
```

### 3 æ¨¡å‹éƒ¨ç½²

#### 3.1 ç›´æ¥é€šè¿‡è„šæœ¬

```python
import os
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['VLLM_TARGET_DEVICE'] = 'cpu'

# æ¨¡å‹ IDï¼šæˆ‘ä»¬ä¸‹è½½çš„æ¨¡å‹æƒé‡æ–‡ä»¶ç›®å½•
model_dir = '/root/modelspace/Qwen2.5-1.5B-Instruct'

# Tokenizer åˆå§‹åŒ–
tokenizer = AutoTokenizer.from_pretrained(
    model_dir,
    local_files_only=True,
)

messages = [
    {'role': 'system', 'content': 'You are a helpful assistant.'},
    {'role': 'user', 'content': 'è·Ÿæˆ‘è®²è®²è‚¡å¸‚è¿è¡Œçš„é€»è¾‘'}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
)

llm = LLM(
    model=model_dir,
    tensor_parallel_size=1,
    device='cpu',
)

sampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=512)

outputs = llm.generate([text], sampling_params)

for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text

    print(f'æç¤ºè¯ï¼š{prompt!r}, å¤§æ¨¡å‹æ¨ç†ç»“æœè¾“å‡ºï¼š{generated_text!r}')
```

ç„¶ååœ¨æ¿€æ´»çš„ ravllm ç¯å¢ƒä¸‹è¿è¡Œæ­¤è„šæœ¬ï¼Œåœ¨ç»“æœæœªè¾“å‡ºä¹‹å‰ï¼Œçœ‹ä¸‹å†…å­˜å’Œ CPU çš„çŠ¶æ€
![cpu-status.png](cpu-status.png)

å¯ä»¥çœ‹åˆ° 16 æ ¸çš„ CPU éƒ½åœ¨åŠªåŠ›çš„å·¥ä½œã€‚

æ¨ç†ç»“æŸï¼Œæ¨ç†ç»“æœå¦‚ä¸‹

```shell
(ravllm) root@local:~/lmcode/25# python local.py
INFO 10-31 08:51:52 importing.py:13] Triton not installed; certain
GPU-related functions will not be available.
WARNING 10-31 08:52:02 config.py:421] Async output processing is
only supported for CUDA, TPU, XPU. Disabling it for other platforms.

INFO 10-31 08:52:02 llm_engine.py:240] Initializing an LLM engine
(v0.6.3.post2.dev76+g51c24c97) with config: model='/root/modelspace/
Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='/root/
modelspace/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False,
tokenizer_mode=auto, revision=None, override_neuron_config=None,
rope_scaling=None, rope_theta=None, tokenizer_revision=None,
trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768,
download_dir=None, load_format=LoadFormat.AUTO,
tensor_parallel_size=1, pipeline_parallel_size=1,
disable_custom_all_reduce=False, quantization=None,
enforce_eager=False, kv_cache_dtype=auto,
quantization_param_path=None, device_config=cpu,
decoding_config=DecodingConfig(guided_decoding_backend='outlines'),
observability_config=ObservabilityConfig(otlp_traces_endpoint=None,
collect_model_forward_time=False, collect_model_execute_time=False),
seed=0, served_model_name=/root/modelspace/Qwen2.5-1.5B-Instruct,
num_scheduler_steps=1, chunked_prefill_enabled=False
multi_step_stream_outputs=True, enable_prefix_caching=False,
use_async_output_proc=False, use_cached_outputs=False,
mm_processor_kwargs=None)

WARNING 10-31 08:52:03 cpu_executor.py:332] CUDA graph is not
supported on CPU, fallback to the eager mode.

WARNING 10-31 08:52:03 cpu_executor.py:362] Environment variable
VLLM_CPU_KVCACHE_SPACE (GB) for CPU backend is not set, using 4 by
default.
INFO 10-31 08:52:03 selector.py:193]
Cannot use _Backend.FLASH_ATTN backend on CPU.
INFO 10-31 08:52:03 selector.py:131] Using Torch SDPA backend.
INFO 10-31 08:52:03 selector.py:193]
Cannot use _Backend.FLASH_ATTN backend on CPU.
INFO 10-31 08:52:03 selector.py:131] Using Torch SDPA backend.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.43it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.43it/s]

INFO 10-31 08:52:05 cpu_executor.py:214] # CPU blocks: 9362
Processed prompts:   0%|                  | 0/1
[00:00<?, ?it/s, eProcessed prompts: 100%|â–ˆ| 1/1
[07:28<00:00 44833s Processed prompts: 100%|â–ˆ| 1/1
[07:28<00:00, 448.33s
Promptæç¤ºè¯:
'<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n
<|im_start|>user\nè·Ÿæˆ‘è®²è®²è‚¡å¸‚è¿è¡Œçš„é€»è¾‘<|im_end|>\n
<|im_start|>assistant\n',
å¤§æ¨¡å‹æ¨ç†è¾“å‡º: 'è‚¡å¸‚æ˜¯ä¸€ä¸ªå¤æ‚çš„å¸‚åœºï¼Œå…¶è¿è¡Œå—åˆ°å¤šç§
å› ç´ çš„å½±å“ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›åŸºæœ¬çš„é€»è¾‘ï¼š\n\n1. ä¾›æ±‚å…³ç³»ï¼šè‚¡ç¥¨ä»·æ ¼é€šå¸¸ç”±ä¾›ç»™å’Œéœ€æ±‚å†³å®šã€‚
å¦‚æœå¸‚åœºä¸Šæœ‰æ›´å¤šçš„äººæƒ³è¦è´­ä¹°è‚¡ç¥¨ï¼Œè€Œä¾›åº”è€…å´æœ‰é™ï¼Œé‚£ä¹ˆè‚¡ç¥¨çš„ä»·æ ¼å°±ä¼šä¸Šå‡ã€‚åä¹‹ï¼Œå¦‚
æœä¾›åº”é‡å¤§äºéœ€æ±‚é‡ï¼Œè‚¡ç¥¨çš„ä»·æ ¼å°±ä¼šä¸‹é™ã€‚\n\n2. å…¬å¸åŸºæœ¬é¢ï¼šå…¬å¸çš„è´¢åŠ¡çŠ¶å†µã€ç›ˆåˆ©èƒ½
åŠ›ã€å¢é•¿æ½œåŠ›ç­‰å› ç´ éƒ½ä¼šå½±å“è‚¡ç¥¨ä»·æ ¼ã€‚æŠ•èµ„è€…ä¼šå…³æ³¨å…¬å¸çš„ç›ˆåˆ©é¢„æµ‹ã€å¸‚ç›ˆç‡ã€è‚¡æ¯æ”¯ä»˜ç­‰
æŒ‡æ ‡ã€‚\n\n3. å¸‚åœºæƒ…ç»ªï¼šå¸‚åœºæƒ…ç»ªä¹Ÿä¼šå½±å“è‚¡ç¥¨ä»·æ ¼ã€‚ä¾‹å¦‚ï¼Œåœ¨ç»æµè¡°é€€æ—¶æœŸï¼ŒæŠ•èµ„è€…å¯èƒ½
ä¼šæ›´åŠ è°¨æ…ï¼Œä»è€Œå¯¼è‡´è‚¡ç¥¨ä»·æ ¼ä¸‹è·Œã€‚è€Œåœ¨ç»æµç¹è£æ—¶æœŸï¼ŒæŠ•èµ„è€…å¯èƒ½ä¼šæ›´åŠ ä¹è§‚ï¼Œå¯¼è‡´è‚¡ç¥¨
ä»·æ ¼ä¸Šæ¶¨ã€‚\n\n4. åˆ©ç‡æ°´å¹³ï¼šåˆ©ç‡æ°´å¹³ä¹Ÿä¼šå½±å“è‚¡ç¥¨ä»·æ ¼ã€‚å½“åˆ©ç‡ä¸Šå‡æ—¶ï¼ŒæŠ•èµ„è€…å¯èƒ½ä¼šæ›´
æ„¿æ„å°†èµ„é‡‘æŠ•èµ„äºå€ºåˆ¸ç­‰å›ºå®šæ”¶ç›Šäº§å“ï¼Œè€Œä¸æ˜¯è‚¡ç¥¨ï¼Œä»è€Œå¯¼è‡´è‚¡ç¥¨ä»·æ ¼ä¸‹è·Œã€‚ç›¸åï¼Œå½“åˆ©ç‡
ä¸‹é™æ—¶ï¼Œè‚¡ç¥¨ä»·æ ¼å¯èƒ½ä¼šä¸Šæ¶¨ã€‚\n\n5. å¤–éƒ¨äº‹ä»¶ï¼šå¤–éƒ¨äº‹ä»¶ï¼Œå¦‚è‡ªç„¶ç¾å®³ã€æ”¿ç­–å˜åŒ–ã€åœ°ç¼˜
æ”¿æ²»å†²çªç­‰ï¼Œä¹Ÿå¯èƒ½å¯¹è‚¡å¸‚äº§ç”Ÿé‡å¤§å½±å“ã€‚\n\néœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›å› ç´ å¹¶ä¸æ˜¯ç‹¬ç«‹çš„ï¼Œå®ƒä»¬
ä¹‹é—´å¯èƒ½å­˜åœ¨ç›¸äº’ä½œç”¨ã€‚æ­¤å¤–ï¼Œè‚¡å¸‚ä»·æ ¼ä¹Ÿä¼šå—åˆ°å…¶ä»–å„ç§å› ç´ çš„å½±å“ï¼Œå¦‚å…¬å¸ä¸šç»©ã€æ–°é—»æŠ¥
é“ã€å¸‚åœºé¢„æœŸç­‰ã€‚å› æ­¤ï¼ŒæŠ•èµ„è€…éœ€è¦ç»¼åˆè€ƒè™‘å„ç§å› ç´ ï¼Œå¹¶ç»“åˆè‡ªå·±çš„æŠ•èµ„ç­–ç•¥è¿›è¡Œå†³ç­–ã€‚'
```
